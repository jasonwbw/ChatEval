# [ChatEval (A Tool for Evaluating Chatbots)](https://chateval.github.io/)
Chatbot evaluation is really hard. There is [no standard](http://www.seas.upenn.edu/~joao/chatbot_human_evaluation.pdf), and this is our attempt to at least address small parts of this issue.

Right now we using [ParlAi](http://parl.ai/) as our framework for data as well as experiments. We used [OpenMNT-py](https://github.com/OpenNMT/OpenNMT-py) for training models. All of our checkpoints will be made publicly available including all configurations. See this [link](http://chatbot-eval-data.s3-accelerate.amazonaws.com/results/available_checkpoints.txt) for checkpoints from the paper. 

Submit your model! Please take a look our submission [form](https://form.jotform.com/80816491184158).

Amazon Mechanical Turk is not free... we are actively looking for funding.

Please find our paper [here](paper/Chatbot_Evaluation_Demo_2018_EMNLP.pdf).

**What does ChatEval solve?**
 1. Shared and publicly available model code and checkpoints.
 1. Standard evaluation datasets.
 1. Standard human annotator framework (currently using Amazon Mechanical Turk).
 1. Model comparisons of the performance of Model A vs Model B. Both a summary and all data are available.

